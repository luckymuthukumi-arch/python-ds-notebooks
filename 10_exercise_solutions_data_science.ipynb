{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10 \u2014 Exercise Solutions (Data Science)\n",
        "\n",
        "Solutions for practice tasks from notebooks 05\u201308."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution: 100 random numbers summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "nums = rng.normal(loc=50, scale=10, size=100)\n",
        "print(\"Min:\", nums.min())\n",
        "print(\"Max:\", nums.max())\n",
        "print(\"Mean:\", nums.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution: Pass/Fail column in pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"name\": [\"Amina\", \"Brian\", \"Chao\", \"Dina\", \"Eli\"],\n",
        "    \"score\": [88, 72, 95, 67, 81],\n",
        "})\n",
        "\n",
        "df[\"result\"] = df[\"score\"].apply(lambda s: \"Pass\" if s >= 75 else \"Fail\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution: Histogram of scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scores = [70, 75, 80, 85, 90, 95]\n",
        "plt.hist(scores, bins=5, edgecolor=\"black\")\n",
        "plt.title(\"Histogram of Scores\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretation: scores are skewed toward higher values in this tiny sample.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution: Compare test sizes in ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"hours\": [1,2,3,4,5,6,7,8,9,10],\n",
        "    \"score\": [50,55,60,63,68,72,78,84,88,93]\n",
        "})\n",
        "\n",
        "for ts in [0.2, 0.3, 0.4]:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df[[\"hours\"]], df[\"score\"], test_size=ts, random_state=42)\n",
        "    model = LinearRegression().fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    print(f\"test_size={ts}: MAE={mean_absolute_error(y_test, pred):.2f}, R2={r2_score(y_test, pred):.3f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}